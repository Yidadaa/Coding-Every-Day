
@misc{williams_learning_nodate,
	title = {Learning {Non}-volumetric {Depth} {Fusion} using {Successive} {Reprojections} {\textbar} {Perceiving} {Systems} - {Max} {Planck} {Institute} for {Intelligent} {Systems}},
	copyright = {CVPR},
	url = {https://is.tuebingen.mpg.de/},
	abstract = {Using computer vision, computer graphics, and machine learning, we teach computers to see people and understand their behavior in complex 3D scenes.},
	language = {en},
	urldate = {2019-06-14},
	author = {Williams, Jon},
	file = {Snapshot:C\:\\Users\\yida\\Zotero\\storage\\IKRJWWK6\\donne2019cvpr.html:text/html;Williams_Learning Non-volumetric Depth Fusion using Successive Reprojections.pdf:C\:\\Users\\yida\\Zotero\\storage\\6FA92JUD\\Williams_Learning Non-volumetric Depth Fusion using Successive Reprojections.pdf:application/pdf}
}

@article{tosi_learning_2019,
	title = {Learning monocular depth estimation infusing traditional stereo knowledge},
	url = {http://arxiv.org/abs/1904.04144},
	abstract = {Depth estimation from a single image represents a fascinating, yet challenging problem with countless applications. Recent works proved that this task could be learned without direct supervision from ground truth labels leveraging image synthesis on sequences or stereo pairs. Focusing on this second case, in this paper we leverage stereo matching in order to improve monocular depth estimation. To this aim we propose monoResMatch, a novel deep architecture designed to infer depth from a single input image by synthesizing features from a different point of view, horizontally aligned with the input image, performing stereo matching between the two cues. In contrast to previous works sharing this rationale, our network is the first trained end-to-end from scratch. Moreover, we show how obtaining proxy ground truth annotation through traditional stereo algorithms, such as Semi-Global Matching, enables more accurate monocular depth estimation still countering the need for expensive depth labels by keeping a self-supervised approach. Exhaustive experimental results prove how the synergy between i) the proposed monoResMatch architecture and ii) proxy-supervision attains state-of-the-art for self-supervised monocular depth estimation. The code is publicly available at https://github.com/fabiotosi92/monoResMatch-Tensorflow.},
	urldate = {2019-06-14},
	journal = {arXiv:1904.04144 [cs]},
	author = {Tosi, Fabio and Aleotti, Filippo and Poggi, Matteo and Mattoccia, Stefano},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.04144},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\yida\\Zotero\\storage\\TK3Y5PC3\\1904.html:text/html;Tosi 等。 - 2019 - Learning monocular depth estimation infusing tradi.pdf:C\:\\Users\\yida\\Zotero\\storage\\KBVR8DSA\\Tosi 等。 - 2019 - Learning monocular depth estimation infusing tradi.pdf:application/pdf}
}

@article{wang_recurrent_nodate,
	title = {Recurrent {Neural} {Network} for ({Un}-){Supervised} {Learning} of {Monocular} {Video} {Visual} {Odometry} and {Depth}},
	abstract = {Deep learning-based, single-view depth estimation methods have recently shown highly promising results. However, such methods ignore one of the most important features for determining depth in the human vision system, which is motion. We propose a learning-based, multiview dense depth map and odometry estimation method that uses Recurrent Neural Networks (RNN) and trains utilizing multi-view image reprojection and forward-backward ﬂowconsistency losses. Our model can be trained in a supervised or even unsupervised mode. It is designed for depth and visual odometry estimation from video where the input frames are temporally correlated. However, it also generalizes to single-view depth estimation. Our method produces superior results to the state-of-the-art approaches for single-view and multi-view learning-based depth estimation on the KITTI driving dataset.},
	language = {en},
	author = {Wang, Rui and Pizer, Stephen M and Frahm, Jan-Michael},
	pages = {10},
	file = {Wang 等。 - Recurrent Neural Network for (Un-)Supervised Learn.pdf:C\:\\Users\\yida\\Zotero\\storage\\CFJVJQYE\\Wang 等。 - Recurrent Neural Network for (Un-)Supervised Learn.pdf:application/pdf}
}

@article{almalioglu_ganvo:_2018,
	title = {{GANVO}: {Unsupervised} {Deep} {Monocular} {Visual} {Odometry} and {Depth} {Estimation} with {Generative} {Adversarial} {Networks}},
	shorttitle = {{GANVO}},
	url = {http://arxiv.org/abs/1809.05786},
	abstract = {In the last decade, supervised deep learning approaches have been extensively employed in visual odometry (VO) applications, which is not feasible in environments where labelled data is not abundant. On the other hand, unsupervised deep learning approaches for localization and mapping in unknown environments from unlabelled data have received comparatively less attention in VO research. In this study, we propose a generative unsupervised learning framework that predicts 6-DoF pose camera motion and monocular depth map of the scene from unlabelled RGB image sequences, using deep convolutional Generative Adversarial Networks (GANs). We create a supervisory signal by warping view sequences and assigning the re-projection minimization to the objective loss function that is adopted in multi-view pose estimation and single-view depth generation network. Detailed quantitative and qualitative evaluations of the proposed framework on the KITTI and Cityscapes datasets show that the proposed method outperforms both existing traditional and unsupervised deep VO methods providing better results for both pose estimation and depth recovery.},
	urldate = {2019-06-14},
	journal = {arXiv:1809.05786 [cs, stat]},
	author = {Almalioglu, Yasin and Saputra, Muhamad Risqi U. and de Gusmao, Pedro P. B. and Markham, Andrew and Trigoni, Niki},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.05786},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1809.05786 PDF:C\:\\Users\\yida\\Zotero\\storage\\VXBGJ47G\\Almalioglu 等。 - 2018 - GANVO Unsupervised Deep Monocular Visual Odometry.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\yida\\Zotero\\storage\\AVS5ER78\\1809.html:text/html}
}

@article{wang_image_2004,
	title = {Image {Quality} {Assessment}: {From} {Error} {Visibility} to {Structural} {Similarity}},
	volume = {13},
	shorttitle = {Image {Quality} {Assessment}},
	doi = {10.1109/TIP.2003.819861},
	abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu/∼lcv/ssim/.},
	journal = {Image Processing, IEEE Transactions on},
	author = {Wang, Zhou and Bovik, Alan and Rahim Sheikh, Hamid and Simoncelli, Eero},
	month = may,
	year = {2004},
	pages = {600--612},
	file = {Full Text PDF:C\:\\Users\\yida\\Zotero\\storage\\CTUAQSV2\\Wang 等。 - 2004 - Image Quality Assessment From Error Visibility to.pdf:application/pdf}
}

@article{godard_digging_2019,
	title = {Digging {Into} {Self}-{Supervised} {Monocular} {Depth} {Estimation}},
	url = {http://arxiv.org/abs/1806.01260},
	abstract = {Per-pixel ground-truth depth data is challenging to acquire at scale. To overcome this limitation, self-supervised learning has emerged as a promising alternative for training models to perform monocular depth estimation. In this paper, we propose a set of improvements, which together result in both quantitatively and qualitatively improved depth maps compared to competing self-supervised methods. Research on self-supervised monocular training usually explores increasingly complex architectures, loss functions, and image formation models, all of which have recently helped to close the gap with fully-supervised methods. We show that a surprisingly simple model, and associated design choices, lead to superior predictions. In particular, we propose (i) a minimum reprojection loss, designed to robustly handle occlusions, (ii) a full-resolution multi-scale sampling method that reduces visual artifacts, and (iii) an auto-masking loss to ignore training pixels that violate camera motion assumptions. We demonstrate the effectiveness of each component in isolation, and show high quality, state-of-the-art results on the KITTI benchmark.},
	urldate = {2019-06-11},
	journal = {arXiv:1806.01260 [cs, stat]},
	author = {Godard, Clément and Mac Aodha, Oisin and Firman, Michael and Brostow, Gabriel},
	month = jun,
	year = {2019},
	note = {arXiv: 1806.01260},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {arXiv\:1806.01260 PDF:C\:\\Users\\yida\\Zotero\\storage\\YPWR4YWP\\Godard 等。 - 2018 - Digging Into Self-Supervised Monocular Depth Estim.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\yida\\Zotero\\storage\\4TH54BNP\\1806.html:text/html}
}

@article{zou_df-net:_2018,
	title = {{DF}-{Net}: {Unsupervised} {Joint} {Learning} of {Depth} and {Flow} using {Cross}-{Task} {Consistency}},
	shorttitle = {{DF}-{Net}},
	url = {http://arxiv.org/abs/1809.01649},
	abstract = {We present an unsupervised learning framework for simultaneously training single-view depth prediction and optical flow estimation models using unlabeled video sequences. Existing unsupervised methods often exploit brightness constancy and spatial smoothness priors to train depth or flow models. In this paper, we propose to leverage geometric consistency as additional supervisory signals. Our core idea is that for rigid regions we can use the predicted scene depth and camera motion to synthesize 2D optical flow by backprojecting the induced 3D scene flow. The discrepancy between the rigid flow (from depth prediction and camera motion) and the estimated flow (from optical flow model) allows us to impose a cross-task consistency loss. While all the networks are jointly optimized during training, they can be applied independently at test time. Extensive experiments demonstrate that our depth and flow models compare favorably with state-of-the-art unsupervised methods.},
	urldate = {2019-06-11},
	journal = {arXiv:1809.01649 [cs]},
	author = {Zou, Yuliang and Luo, Zelun and Huang, Jia-Bin},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.01649},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 光流, 深度估计},
	file = {arXiv\:1809.01649 PDF:C\:\\Users\\yida\\Zotero\\storage\\5CEAY2ZC\\Zou 等。 - 2018 - DF-Net Unsupervised Joint Learning of Depth and F.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\yida\\Zotero\\storage\\RH3VUAW2\\1809.html:text/html}
}

@inproceedings{yin_geonet:_2018,
	address = {Salt Lake City, UT},
	title = {{GeoNet}: {Unsupervised} {Learning} of {Dense} {Depth}, {Optical} {Flow} and {Camera} {Pose}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{GeoNet}},
	url = {https://ieeexplore.ieee.org/document/8578310/},
	doi = {10.1109/CVPR.2018.00212},
	abstract = {We propose GeoNet, a jointly unsupervised learning framework for monocular depth, optical ﬂow and egomotion estimation from videos. The three components are coupled by the nature of 3D scene geometry, jointly learned by our framework in an end-to-end manner. Speciﬁcally, geometric relationships are extracted over the predictions of individual modules and then combined as an image reconstruction loss, reasoning about static and dynamic scene parts separately. Furthermore, we propose an adaptive geometric consistency loss to increase robustness towards outliers and non-Lambertian regions, which resolves occlusions and texture ambiguities effectively. Experimentation on the KITTI driving dataset reveals that our scheme achieves state-of-the-art results in all of the three tasks, performing better than previously unsupervised methods and comparably with supervised ones.},
	language = {en},
	urldate = {2019-06-11},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Yin, Zhichao and Shi, Jianping},
	month = jun,
	year = {2018},
	keywords = {光流, 深度估计},
	pages = {1983--1992},
	file = {Yin 和 Shi - 2018 - GeoNet Unsupervised Learning of Dense Depth, Opti.pdf:C\:\\Users\\yida\\Zotero\\storage\\KEQ9YFPM\\Yin 和 Shi - 2018 - GeoNet Unsupervised Learning of Dense Depth, Opti.pdf:application/pdf}
}

@article{eigen_predicting_2014,
	title = {Predicting {Depth}, {Surface} {Normals} and {Semantic} {Labels} with a {Common} {Multi}-{Scale} {Convolutional} {Architecture}},
	url = {http://arxiv.org/abs/1411.4734},
	abstract = {In this paper we address three different computer vision tasks using a single basic architecture: depth prediction, surface normal estimation, and semantic labeling. We use a multiscale convolutional network that is able to adapt easily to each task using only small modifications, regressing from the input image to the output map directly. Our method progressively refines predictions using a sequence of scales, and captures many image details without any superpixels or low-level segmentation. We achieve state-of-the-art performance on benchmarks for all three tasks.},
	urldate = {2019-06-11},
	journal = {arXiv:1411.4734 [cs]},
	author = {Eigen, David and Fergus, Rob},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.4734},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1411.4734 PDF:C\:\\Users\\yida\\Zotero\\storage\\IWXAEBL7\\Eigen 和 Fergus - 2014 - Predicting Depth, Surface Normals and Semantic Lab.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\yida\\Zotero\\storage\\ERHBCAUA\\1411.html:text/html}
}

@article{eigen_depth_2014,
	title = {Depth {Map} {Prediction} from a {Single} {Image} using a {Multi}-{Scale} {Deep} {Network}},
	url = {http://arxiv.org/abs/1406.2283},
	abstract = {Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.},
	urldate = {2019-06-11},
	journal = {arXiv:1406.2283 [cs]},
	author = {Eigen, David and Puhrsch, Christian and Fergus, Rob},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2283},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1406.2283 PDF:C\:\\Users\\yida\\Zotero\\storage\\99M24PPX\\Eigen 等。 - 2014 - Depth Map Prediction from a Single Image using a M.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\yida\\Zotero\\storage\\JKFWSPVR\\1406.html:text/html}
}

@article{xu_region_2019,
	title = {Region {Deformer} {Networks} for {Unsupervised} {Depth} {Estimation} from {Unconstrained} {Monocular} {Videos}},
	url = {http://arxiv.org/abs/1902.09907},
	abstract = {While learning based depth estimation from images/videos has achieved substantial progress, there still exist intrinsic limitations. Supervised methods are limited by a small amount of ground truth or labeled data and unsupervised methods for monocular videos are mostly based on the static scene assumption, not performing well on real world scenarios with the presence of dynamic objects. In this paper, we propose a new learning based method consisting of DepthNet, PoseNet and Region Deformer Networks (RDN) to estimate depth from unconstrained monocular videos without ground truth supervision. The core contribution lies in RDN for proper handling of rigid and non-rigid motions of various objects such as rigidly moving cars and deformable humans. In particular, a deformation based motion representation is proposed to model individual object motion on 2D images. This representation enables our method to be applicable to diverse unconstrained monocular videos. Our method can not only achieve the state-of-the-art results on standard benchmarks KITTI and Cityscapes, but also show promising results on a crowded pedestrian tracking dataset, which demonstrates the effectiveness of the deformation based motion representation. Code and trained models are available at https://github.com/haofeixu/rdn4depth.},
	urldate = {2019-06-11},
	journal = {arXiv:1902.09907 [cs]},
	author = {Xu, Haofei and Zheng, Jianmin and Cai, Jianfei and Zhang, Juyong},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.09907},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1902.09907 PDF:C\:\\Users\\yida\\Zotero\\storage\\FFE4BYYA\\Xu 等。 - 2019 - Region Deformer Networks for Unsupervised Depth Es.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\yida\\Zotero\\storage\\GKXK5Q5I\\1902.html:text/html}
}

@article{yao_recurrent_2019,
	title = {Recurrent {MVSNet} for {High}-resolution {Multi}-view {Stereo} {Depth} {Inference}},
	url = {http://arxiv.org/abs/1902.10556},
	abstract = {Deep learning has recently demonstrated its excellent performance for multi-view stereo (MVS). However, one major limitation of current learned MVS approaches is the scalability: the memory-consuming cost volume regularization makes the learned MVS hard to be applied to high-resolution scenes. In this paper, we introduce a scalable multi-view stereo framework based on the recurrent neural network. Instead of regularizing the entire 3D cost volume in one go, the proposed Recurrent Multi-view Stereo Network (R-MVSNet) sequentially regularizes the 2D cost maps along the depth direction via the gated recurrent unit (GRU). This reduces dramatically the memory consumption and makes high-resolution reconstruction feasible. We first show the state-of-the-art performance achieved by the proposed R-MVSNet on the recent MVS benchmarks. Then, we further demonstrate the scalability of the proposed method on several large-scale scenarios, where previous learned approaches often fail due to the memory constraint. Code is available at https://github.com/YoYo000/MVSNet.},
	urldate = {2019-06-04},
	journal = {arXiv:1902.10556 [cs]},
	author = {Yao, Yao and Luo, Zixin and Li, Shiwei and Shen, Tianwei and Fang, Tian and Quan, Long},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.10556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1902.10556 PDF:C\:\\Users\\yida\\Zotero\\storage\\KQTFWF6Z\\Yao 等。 - 2019 - Recurrent MVSNet for High-resolution Multi-view St.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\yida\\Zotero\\storage\\HZYRDCQU\\1902.html:text/html}
}

@article{zhan_unsupervised_2018,
	title = {Unsupervised {Learning} of {Monocular} {Depth} {Estimation} and {Visual} {Odometry} with {Deep} {Feature} {Reconstruction}},
	url = {http://arxiv.org/abs/1803.03893},
	abstract = {Despite learning based methods showing promising results in single view depth estimation and visual odometry, most existing approaches treat the tasks in a supervised manner. Recent approaches to single view depth estimation explore the possibility of learning without full supervision via minimizing photometric error. In this paper, we explore the use of stereo sequences for learning depth and visual odometry. The use of stereo sequences enables the use of both spatial (between left-right pairs) and temporal (forward backward) photometric warp error, and constrains the scene depth and camera motion to be in a common, real-world scale. At test time our framework is able to estimate single view depth and two-view odometry from a monocular sequence. We also show how we can improve on a standard photometric warp loss by considering a warp of deep features. We show through extensive experiments that: (i) jointly training for single view depth and visual odometry improves depth prediction because of the additional constraint imposed on depths and achieves competitive results for visual odometry; (ii) deep feature-based warping loss improves upon simple photometric warp loss for both single view depth estimation and visual odometry. Our method outperforms existing learning based methods on the KITTI driving dataset in both tasks. The source code is available at https://github.com/Huangying-Zhan/Depth-VO-Feat},
	urldate = {2019-06-04},
	journal = {arXiv:1803.03893 [cs]},
	author = {Zhan, Huangying and Garg, Ravi and Weerasekera, Chamara Saroj and Li, Kejie and Agarwal, Harsh and Reid, Ian},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.03893},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1803.03893 PDF:C\:\\Users\\yida\\Zotero\\storage\\CSTVHKLD\\Zhan 等。 - 2018 - Unsupervised Learning of Monocular Depth Estimatio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\yida\\Zotero\\storage\\SN2C5HBZ\\1803.html:text/html}
}

@article{atapour-abarghouei_veritatem_2019,
	title = {Veritatem {Dies} {Aperit}- {Temporally} {Consistent} {Depth} {Prediction} {Enabled} by a {Multi}-{Task} {Geometric} and {Semantic} {Scene} {Understanding} {Approach}},
	url = {http://arxiv.org/abs/1903.10764},
	abstract = {Robust geometric and semantic scene understanding is ever more important in many real-world applications such as autonomous driving and robotic navigation. In this paper, we propose a multi-task learning-based approach capable of jointly performing geometric and semantic scene understanding, namely depth prediction (monocular depth estimation and depth completion) and semantic scene segmentation. Within a single temporally constrained recurrent network, our approach uniquely takes advantage of a complex series of skip connections, adversarial training and the temporal constraint of sequential frame recurrence to produce consistent depth and semantic class labels simultaneously. Extensive experimental evaluation demonstrates the efficacy of our approach compared to other contemporary state-of-the-art techniques.},
	urldate = {2019-06-04},
	journal = {arXiv:1903.10764 [cs]},
	author = {Atapour-Abarghouei, Amir and Breckon, Toby P.},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.10764},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1903.10764 PDF:C\:\\Users\\yida\\Zotero\\storage\\MQFY7BP3\\Atapour-Abarghouei 和 Breckon - 2019 - Veritatem Dies Aperit- Temporally Consistent Depth.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\yida\\Zotero\\storage\\Q9H7VEWY\\1903.html:text/html}
}

@article{ranjan_competitive_2018,
	title = {Competitive {Collaboration}: {Joint} {Unsupervised} {Learning} of {Depth}, {Camera} {Motion}, {Optical} {Flow} and {Motion} {Segmentation}},
	shorttitle = {Competitive {Collaboration}},
	url = {http://arxiv.org/abs/1805.09806},
	abstract = {We address the unsupervised learning of several interconnected problems in low-level vision: single view depth prediction, camera motion estimation, optical flow, and segmentation of a video into the static scene and moving regions. Our key insight is that these four fundamental vision problems are coupled through geometric constraints. Consequently, learning to solve them together simplifies the problem because the solutions can reinforce each other. We go beyond previous work by exploiting geometry more explicitly and segmenting the scene into static and moving regions. To that end, we introduce Competitive Collaboration, a framework that facilitates the coordinated training of multiple specialized neural networks to solve complex problems. Competitive Collaboration works much like expectation-maximization, but with neural networks that act as both competitors to explain pixels that correspond to static or moving regions, and as collaborators through a moderator that assigns pixels to be either static or independently moving. Our novel method integrates all these problems in a common framework and simultaneously reasons about the segmentation of the scene into moving objects and the static background, the camera motion, depth of the static scene structure, and the optical flow of moving objects. Our model is trained without any supervision and achieves state-of-the-art performance among joint unsupervised methods on all sub-problems.},
	urldate = {2019-06-04},
	journal = {arXiv:1805.09806 [cs]},
	author = {Ranjan, Anurag and Jampani, Varun and Balles, Lukas and Kim, Kihwan and Sun, Deqing and Wulff, Jonas and Black, Michael J.},
	month = may,
	year = {2018},
	note = {arXiv: 1805.09806},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 光流, 深度估计},
	file = {arXiv\:1805.09806 PDF:C\:\\Users\\yida\\Zotero\\storage\\9BFQCEJT\\Ranjan 等。 - 2018 - Competitive Collaboration Joint Unsupervised Lear.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\yida\\Zotero\\storage\\RBGIGFFB\\1805.html:text/html;Ranjan et al_2018_Competitive Collaboration.pdf:C\:\\Users\\yida\\Zotero\\storage\\4KX4KK4U\\Ranjan et al_2018_Competitive Collaboration.pdf:application/pdf}
}

@article{luo_single_2018,
	title = {Single {View} {Stereo} {Matching}},
	url = {http://arxiv.org/abs/1803.02612},
	abstract = {Previous monocular depth estimation methods take a single view and directly regress the expected results. Though recent advances are made by applying geometrically inspired loss functions during training, the inference procedure does not explicitly impose any geometrical constraint. Therefore these models purely rely on the quality of data and the effectiveness of learning to generalize. This either leads to suboptimal results or the demand of huge amount of expensive ground truth labelled data to generate reasonable results. In this paper, we show for the first time that the monocular depth estimation problem can be reformulated as two sub-problems, a view synthesis procedure followed by stereo matching, with two intriguing properties, namely i) geometrical constraints can be explicitly imposed during inference; ii) demand on labelled depth data can be greatly alleviated. We show that the whole pipeline can still be trained in an end-to-end fashion and this new formulation plays a critical role in advancing the performance. The resulting model outperforms all the previous monocular depth estimation methods as well as the stereo block matching method in the challenging KITTI dataset by only using a small number of real training data. The model also generalizes well to other monocular depth estimation benchmarks. We also discuss the implications and the advantages of solving monocular depth estimation using stereo methods.},
	urldate = {2019-04-28},
	journal = {arXiv:1803.02612 [cs]},
	author = {Luo, Yue and Ren, Jimmy and Lin, Mude and Pang, Jiahao and Sun, Wenxiu and Li, Hongsheng and Lin, Liang},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.02612},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1803.02612 PDF:C\:\\Users\\yida\\Zotero\\storage\\ER8LLYWX\\Luo 等。 - 2018 - Single View Stereo Matching.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\yida\\Zotero\\storage\\LB47LUQN\\1803.html:text/html}
}

@article{cho_large_2019,
	title = {A {Large} {RGB}-{D} {Dataset} for {Semi}-supervised {Monocular} {Depth} {Estimation}},
	url = {http://arxiv.org/abs/1904.10230},
	abstract = {The recent advance of monocular depth estimation is largely based on deeply nested convolutional networks, combined with supervised training. However, it still remains arduous to collect large-scale ground truth depth (or disparity) maps for supervising the networks. This paper presents a simple yet effective semi-supervised approach for monocular depth estimation. Inspired by the human visual system, we propose a student-teacher strategy in which a shallow student network is trained with the auxiliary information obtained from a deeper and accurate teacher network. Specifically, we first train the stereo teacher network fully utilizing the binocular perception of 3D geometry, and then use depth predictions of the teacher network for supervising the student network for monocular depth inference. This enables us to exploit all available depth data from massive unlabeled stereo pairs that are relatively easier-to-obtain. We further introduce a data ensemble strategy that merges multiple depth predictions of the teacher network to improve the training samples for the student network. Additionally, stereo confidence maps are provided to avoid inaccurate depth estimates being used when supervising the student network. Our new training data, consisting of 1 million outdoor stereo images taken using hand-held stereo cameras, is hosted at the project webpage. Lastly, we demonstrate that the monocular depth estimation network provides feature representations that are suitable for some high-level vision tasks such as semantic segmentation and road detection. Extensive experiments demonstrate the effectiveness and flexibility of the proposed method in various outdoor scenarios.},
	urldate = {2019-04-26},
	journal = {arXiv:1904.10230 [cs]},
	author = {Cho, Jaehoon and Min, Dongbo and Kim, Youngjung and Sohn, Kwanghoon},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.10230},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1904.10230 PDF:C\:\\Users\\yida\\Zotero\\storage\\JSIVYT9M\\Cho 等。 - 2019 - A Large RGB-D Dataset for Semi-supervised Monocula.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\yida\\Zotero\\storage\\JBEJRKC9\\1904.html:text/html}
}

@article{wang_pseudo-lidar_2018,
	title = {Pseudo-{LiDAR} from {Visual} {Depth} {Estimation}: {Bridging} the {Gap} in 3D {Object} {Detection} for {Autonomous} {Driving}},
	shorttitle = {Pseudo-{LiDAR} from {Visual} {Depth} {Estimation}},
	url = {http://arxiv.org/abs/1812.07179},
	abstract = {3D object detection is an essential task in autonomous driving. Recent techniques excel with highly accurate detection rates, provided the 3D input data is obtained from precise but expensive LiDAR technology. Approaches based on cheaper monocular or stereo imagery data have, until now, resulted in drastically lower accuracies --- a gap that is commonly attributed to poor image-based depth estimation. However, in this paper we argue that it is not the quality of the data but its representation that accounts for the majority of the difference. Taking the inner workings of convolutional neural networks into consideration, we propose to convert image-based depth maps to pseudo-LiDAR representations --- essentially mimicking the LiDAR signal. With this representation we can apply different existing LiDAR-based detection algorithms. On the popular KITTI benchmark, our approach achieves impressive improvements over the existing state-of-the-art in image-based performance --- raising the detection accuracy of objects within the 30m range from the previous state-of-the-art of 22\% to an unprecedented 74\%. At the time of submission our algorithm holds the highest entry on the KITTI 3D object detection leaderboard for stereo-image-based approaches.},
	urldate = {2019-04-26},
	journal = {arXiv:1812.07179 [cs]},
	author = {Wang, Yan and Chao, Wei-Lun and Garg, Divyansh and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q.},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.07179},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1812.07179 PDF:C\:\\Users\\yida\\Zotero\\storage\\KPZIB6PH\\Wang 等。 - 2018 - Pseudo-LiDAR from Visual Depth Estimation Bridgin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\yida\\Zotero\\storage\\3JIG39PS\\1812.html:text/html}
}

@inproceedings{flynn_deep_2016,
	address = {Las Vegas, NV, USA},
	title = {Deep {Stereo}: {Learning} to {Predict} {New} {Views} from the {World}'s {Imagery}},
	isbn = {978-1-4673-8851-1},
	shorttitle = {Deep {Stereo}},
	url = {http://ieeexplore.ieee.org/document/7780964/},
	doi = {10.1109/CVPR.2016.595},
	abstract = {Deep networks have recently enjoyed enormous success when applied to recognition and classiﬁcation problems in computer vision [22, 33], but their use in graphics problems has been limited ([23, 7] are notable recent exceptions). In this work, we present a novel deep architecture that performs new view synthesis directly from pixels, trained from a large number of posed image sets. In contrast to traditional approaches, which consist of multiple complex stages of processing, each of which requires careful tuning and can fail in unexpected ways, our system is trained end-to-end. The pixels from neighboring views of a scene are presented to the network, which then directly produces the pixels of the unseen view. The beneﬁts of our approach include generality (we only require posed image sets and can easily apply our method to different domains), and high quality results on traditionally difﬁcult scenes. We believe this is due to the end-to-end nature of our system, which is able to plausibly generate pixels according to color, depth, and texture priors learnt automatically from the training data. We show view interpolation results on imagery from the KITTI dataset [12], from data from [1] as well as on Google Street View images. To our knowledge, our work is the ﬁrst to apply deep learning to the problem of new view synthesis from sets of real-world, natural imagery.},
	language = {en},
	urldate = {2019-04-11},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Flynn, John and Neulander, Ivan and Philbin, James and Snavely, Noah},
	month = jun,
	year = {2016},
	pages = {5515--5524},
	file = {Flynn 等。 - 2016 - Deep Stereo Learning to Predict New Views from th.pdf:C\:\\Users\\yida\\Zotero\\storage\\42T35IY3\\Flynn 等。 - 2016 - Deep Stereo Learning to Predict New Views from th.pdf:application/pdf}
}

@article{garg_unsupervised_2016,
	title = {Unsupervised {CNN} for {Single} {View} {Depth} {Estimation}: {Geometry} to the {Rescue}},
	shorttitle = {Unsupervised {CNN} for {Single} {View} {Depth} {Estimation}},
	url = {http://arxiv.org/abs/1603.04992},
	abstract = {A significant weakness of most current deep Convolutional Neural Networks is the need to train them using vast amounts of manu- ally labelled data. In this work we propose a unsupervised framework to learn a deep convolutional neural network for single view depth predic- tion, without requiring a pre-training stage or annotated ground truth depths. We achieve this by training the network in a manner analogous to an autoencoder. At training time we consider a pair of images, source and target, with small, known camera motion between the two such as a stereo pair. We train the convolutional encoder for the task of predicting the depth map for the source image. To do so, we explicitly generate an inverse warp of the target image using the predicted depth and known inter-view displacement, to reconstruct the source image; the photomet- ric error in the reconstruction is the reconstruction loss for the encoder. The acquisition of this training data is considerably simpler than for equivalent systems, requiring no manual annotation, nor calibration of depth sensor to camera. We show that our network trained on less than half of the KITTI dataset (without any further augmentation) gives com- parable performance to that of the state of art supervised methods for single view depth estimation.},
	urldate = {2019-04-11},
	journal = {arXiv:1603.04992 [cs]},
	author = {Garg, Ravi and BG, Vijay Kumar and Carneiro, Gustavo and Reid, Ian},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.04992},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1603.04992 PDF:C\:\\Users\\yida\\Zotero\\storage\\IMU3AN8C\\Garg 等。 - 2016 - Unsupervised CNN for Single View Depth Estimation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\yida\\Zotero\\storage\\QS5V6NPC\\1603.html:text/html}
}

@inproceedings{godard_unsupervised_2017,
	address = {Honolulu, HI},
	title = {Unsupervised {Monocular} {Depth} {Estimation} with {Left}-{Right} {Consistency}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8100182/},
	doi = {10.1109/CVPR.2017.699},
	abstract = {Learning based methods have shown very promising results for the task of depth estimation in single images. However, most existing approaches treat depth prediction as a supervised regression problem and as a result, require vast quantities of corresponding ground truth depth data for training. Just recording quality depth data in a range of environments is a challenging problem. In this paper, we innovate beyond existing approaches, replacing the use of explicit depth data during training with easier-to-obtain binocular stereo footage.},
	language = {en},
	urldate = {2019-04-11},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Godard, Clement and Aodha, Oisin Mac and Brostow, Gabriel J.},
	month = jul,
	year = {2017},
	pages = {6602--6611},
	file = {Godard 等。 - 2017 - Unsupervised Monocular Depth Estimation with Left-.pdf:C\:\\Users\\yida\\Zotero\\storage\\8KG3DWAU\\Godard 等。 - 2017 - Unsupervised Monocular Depth Estimation with Left-.pdf:application/pdf}
}

@inproceedings{zhou_unsupervised_2017,
	address = {Honolulu, HI},
	title = {Unsupervised {Learning} of {Depth} and {Ego}-{Motion} from {Video}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8100183/},
	doi = {10.1109/CVPR.2017.700},
	abstract = {We present an unsupervised learning framework for the task of monocular depth and camera motion estimation from unstructured video sequences. In common with recent work [10, 14, 16], we use an end-to-end learning approach with view synthesis as the supervisory signal. In contrast to the previous work, our method is completely unsupervised, requiring only monocular video sequences for training. Our method uses single-view depth and multiview pose networks, with a loss based on warping nearby views to the target using the computed depth and pose. The networks are thus coupled by the loss during training, but can be applied independently at test time. Empirical evaluation on the KITTI dataset demonstrates the effectiveness of our approach: 1) monocular depth performs comparably with supervised methods that use either ground-truth pose or depth for training, and 2) pose estimation performs favorably compared to established SLAM systems under comparable input settings.},
	language = {en},
	urldate = {2019-04-11},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhou, Tinghui and Brown, Matthew and Snavely, Noah and Lowe, David G.},
	month = jul,
	year = {2017},
	pages = {6612--6619},
	file = {Zhou 等。 - 2017 - Unsupervised Learning of Depth and Ego-Motion from.pdf:C\:\\Users\\yida\\Zotero\\storage\\XFMS5ANC\\Zhou 等。 - 2017 - Unsupervised Learning of Depth and Ego-Motion from.pdf:application/pdf}
}

@article{mahjourian_unsupervised_2018,
	title = {Unsupervised {Learning} of {Depth} and {Ego}-{Motion} from {Monocular} {Video} {Using} 3D {Geometric} {Constraints}},
	url = {http://arxiv.org/abs/1802.05522},
	abstract = {We present a novel approach for unsupervised learning of depth and ego-motion from monocular video. Unsupervised learning removes the need for separate supervisory signals (depth or ego-motion ground truth, or multi-view video). Prior work in unsupervised depth learning uses pixel-wise or gradient-based losses, which only consider pixels in small local neighborhoods. Our main contribution is to explicitly consider the inferred 3D geometry of the whole scene, and enforce consistency of the estimated 3D point clouds and ego-motion across consecutive frames. This is a challenging task and is solved by a novel (approximate) backpropagation algorithm for aligning 3D structures.},
	language = {en},
	urldate = {2019-04-08},
	journal = {arXiv:1802.05522 [cs]},
	author = {Mahjourian, Reza and Wicke, Martin and Angelova, Anelia},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.05522},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, vid2depth},
	file = {Mahjourian 等。 - 2018 - Unsupervised Learning of Depth and Ego-Motion from.pdf:C\:\\Users\\yida\\Zotero\\storage\\TGLSS3FS\\Mahjourian 等。 - 2018 - Unsupervised Learning of Depth and Ego-Motion from.pdf:application/pdf}
}

@inproceedings{wang_learning_2018,
	address = {Salt Lake City, UT, USA},
	title = {Learning {Depth} from {Monocular} {Videos} {Using} {Direct} {Methods}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578314/},
	doi = {10.1109/CVPR.2018.00216},
	abstract = {The ability to predict depth from a single image - using recent advances in CNNs - is of increasing interest to the vision community. Unsupervised strategies to learning are particularly appealing as they can utilize much larger and varied monocular video datasets during learning without the need for ground truth depth or stereo. In previous works, separate pose and depth CNN predictors had to be determined such that their joint outputs minimized the photometric error. Inspired by recent advances in direct visual odometry (DVO), we argue that the depth CNN predictor can be learned without a pose CNN predictor. Further, we demonstrate empirically that incorporation of a differentiable implementation of DVO, along with a novel depth normalization strategy - substantially improves performance over state of the art that use monocular videos for training.},
	language = {en},
	urldate = {2019-04-08},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Wang, Chaoyang and Buenaposada, Jose Miguel and Zhu, Rui and Lucey, Simon},
	month = jun,
	year = {2018},
	pages = {2022--2030},
	file = {Wang 等。 - 2018 - Learning Depth from Monocular Videos Using Direct .pdf:C\:\\Users\\yida\\Zotero\\storage\\W4XEZ4BU\\Wang 等。 - 2018 - Learning Depth from Monocular Videos Using Direct .pdf:application/pdf}
}

@article{casser_depth_2018,
	title = {Depth {Prediction} {Without} the {Sensors}: {Leveraging} {Structure} for {Unsupervised} {Learning} from {Monocular} {Videos}},
	shorttitle = {Depth {Prediction} {Without} the {Sensors}},
	url = {http://arxiv.org/abs/1811.06152},
	abstract = {Learning to predict scene depth from RGB inputs is a challenging task both for indoor and outdoor robot navigation. In this work we address unsupervised learning of scene depth and robot ego-motion where supervision is provided by monocular videos, as cameras are the cheapest, least restrictive and most ubiquitous sensor for robotics.},
	language = {en},
	urldate = {2019-04-08},
	journal = {arXiv:1811.06152 [cs]},
	author = {Casser, Vincent and Pirk, Soeren and Mahjourian, Reza and Angelova, Anelia},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.06152},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, AAAI2019, struct2depth},
	file = {Casser 等。 - 2018 - Depth Prediction Without the Sensors Leveraging S.pdf:C\:\\Users\\yida\\Zotero\\storage\\KBYN7AQ2\\Casser 等。 - 2018 - Depth Prediction Without the Sensors Leveraging S.pdf:application/pdf}
}

@article{liu_neural_2019,
	title = {Neural {RGB}-{\textgreater}{D} {Sensing}: {Depth} and {Uncertainty} from a {Video} {Camera}},
	shorttitle = {Neural {RGB}-{\textgreater}{D} {Sensing}},
	url = {http://arxiv.org/abs/1901.02571},
	abstract = {Depth sensing is crucial for 3D reconstruction and scene understanding. Active depth sensors provide dense metric measurements, but often suffer from limitations such as restricted operating ranges, low spatial resolution, sensor interference, and high power consumption. In this paper, we propose a deep learning (DL) method to estimate per-pixel depth and its uncertainty continuously from a monocular video stream, with the goal of effectively turning an RGB camera into an RGB-D camera. Unlike prior DL-based methods, we estimate a depth probability distribution for each pixel rather than a single depth value, leading to an estimate of a 3D depth probability volume for each input frame. These depth probability volumes are accumulated over time under a Bayesian filtering framework as more incoming frames are processed sequentially, which effectively reduces depth uncertainty and improves accuracy, robustness, and temporal stability. Compared to prior work, the proposed approach achieves more accurate and stable results, and generalizes better to new datasets. Experimental results also show the output of our approach can be directly fed into classical RGB-D based 3D scanning methods for 3D scene reconstruction.},
	urldate = {2019-04-08},
	journal = {arXiv:1901.02571 [cs]},
	author = {Liu, Chao and Gu, Jinwei and Kim, Kihwan and Narasimhan, Srinivasa and Kautz, Jan},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.02571},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1901.02571 PDF:C\:\\Users\\yida\\Zotero\\storage\\ZGICPF93\\Liu 等。 - 2019 - Neural RGB-D Sensing Depth and Uncertainty from .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\yida\\Zotero\\storage\\67XU7ARR\\1901.html:text/html}
}

@article{yang_lego:_2018,
	title = {{LEGO}: {Learning} {Edge} with {Geometry} all at {Once} by {Watching} {Videos}},
	shorttitle = {{LEGO}},
	url = {http://arxiv.org/abs/1803.05648},
	abstract = {Learning to estimate 3D geometry in a single image by watching unlabeled videos via deep convolutional network is attracting significant attention. In this paper, we introduce a "3D as-smooth-as-possible (3D-ASAP)" prior inside the pipeline, which enables joint estimation of edges and 3D scene, yielding results with significant improvement in accuracy for fine detailed structures. Specifically, we define the 3D-ASAP prior by requiring that any two points recovered in 3D from an image should lie on an existing planar surface if no other cues provided. We design an unsupervised framework that Learns Edges and Geometry (depth, normal) all at Once (LEGO). The predicted edges are embedded into depth and surface normal smoothness terms, where pixels without edges in-between are constrained to satisfy the prior. In our framework, the predicted depths, normals and edges are forced to be consistent all the time. We conduct experiments on KITTI to evaluate our estimated geometry and CityScapes to perform edge evaluation. We show that in all of the tasks, i.e.depth, normal and edge, our algorithm vastly outperforms other state-of-the-art (SOTA) algorithms, demonstrating the benefits of our approach.},
	urldate = {2019-06-15},
	journal = {arXiv:1803.05648 [cs]},
	author = {Yang, Zhenheng and Wang, Peng and Wang, Yang and Xu, Wei and Nevatia, Ram},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.05648},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\yida\\Zotero\\storage\\FP9EEC47\\1803.html:text/html;Yang et al_2018_LEGO.pdf:C\:\\Users\\yida\\Zotero\\storage\\GLKHQQR7\\Yang et al_2018_LEGO.pdf:application/pdf}
}

@article{wang_joint_2018,
	title = {Joint {Unsupervised} {Learning} of {Optical} {Flow} and {Depth} by {Watching} {Stereo} {Videos}},
	url = {http://arxiv.org/abs/1810.03654},
	abstract = {Learning depth and optical flow via deep neural networks by watching videos has made significant progress recently. In this paper, we jointly solve the two tasks by exploiting the underlying geometric rules within stereo videos. Specifically, given two consecutive stereo image pairs from a video, we first estimate depth, camera ego-motion and optical flow from three neural networks. Then the whole scene is decomposed into moving foreground and static background by compar- ing the estimated optical flow and rigid flow derived from the depth and ego-motion. We propose a novel consistency loss to let the optical flow learn from the more accurate rigid flow in static regions. We also design a rigid alignment module which helps refine ego-motion estimation by using the estimated depth and optical flow. Experiments on the KITTI dataset show that our results significantly outperform other state-of- the-art algorithms. Source codes can be found at https: //github.com/baidu-research/UnDepthflow},
	urldate = {2019-06-15},
	journal = {arXiv:1810.03654 [cs]},
	author = {Wang, Yang and Yang, Zhenheng and Wang, Peng and Yang, Yi and Luo, Chenxu and Xu, Wei},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.03654},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 光流, 深度估计},
	file = {arXiv.org Snapshot:C\:\\Users\\yida\\Zotero\\storage\\XZARSURW\\1810.html:text/html;Wang et al_2018_Joint Unsupervised Learning of Optical Flow and Depth by Watching Stereo Videos.pdf:C\:\\Users\\yida\\Zotero\\storage\\PCHT8XSX\\Wang et al_2018_Joint Unsupervised Learning of Optical Flow and Depth by Watching Stereo Videos.pdf:application/pdf}
}

@article{yang_every_2018,
	title = {Every {Pixel} {Counts}: {Unsupervised} {Geometry} {Learning} with {Holistic} 3D {Motion} {Understanding}},
	shorttitle = {Every {Pixel} {Counts}},
	url = {http://arxiv.org/abs/1806.10556},
	abstract = {Learning to estimate 3D geometry in a single image by watching unlabeled videos via deep convolutional network has made significant process recently. Current state-of-the-art (SOTA) methods, are based on the learning framework of rigid structure-from-motion, where only 3D camera ego motion is modeled for geometry estimation.However, moving objects also exist in many videos, e.g. moving cars in a street scene. In this paper, we tackle such motion by additionally incorporating per-pixel 3D object motion into the learning framework, which provides holistic 3D scene flow understanding and helps single image geometry estimation. Specifically, given two consecutive frames from a video, we adopt a motion network to predict their relative 3D camera pose and a segmentation mask distinguishing moving objects and rigid background. An optical flow network is used to estimate dense 2D per-pixel correspondence. A single image depth network predicts depth maps for both images. The four types of information, i.e. 2D flow, camera pose, segment mask and depth maps, are integrated into a differentiable holistic 3D motion parser (HMP), where per-pixel 3D motion for rigid background and moving objects are recovered. We design various losses w.r.t. the two types of 3D motions for training the depth and motion networks, yielding further error reduction for estimated geometry. Finally, in order to solve the 3D motion confusion from monocular videos, we combine stereo images into joint training. Experiments on KITTI 2015 dataset show that our estimated geometry, 3D motion and moving object masks, not only are constrained to be consistent, but also significantly outperforms other SOTA algorithms, demonstrating the benefits of our approach.},
	urldate = {2019-06-15},
	journal = {arXiv:1806.10556 [cs]},
	author = {Yang, Zhenheng and Wang, Peng and Wang, Yang and Xu, Wei and Nevatia, Ram},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.10556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\yida\\Zotero\\storage\\8BKU2PDV\\1806.html:text/html;Yang et al_2018_Every Pixel Counts.pdf:C\:\\Users\\yida\\Zotero\\storage\\NMCXG8AZ\\Yang et al_2018_Every Pixel Counts.pdf:application/pdf}
}

@article{luo_every_2018,
	title = {Every {Pixel} {Counts} ++: {Joint} {Learning} of {Geometry} and {Motion} with 3D {Holistic} {Understanding}},
	shorttitle = {Every {Pixel} {Counts} ++},
	url = {http://arxiv.org/abs/1810.06125},
	abstract = {Learning to estimate 3D geometry in a single frame and optical flow from consecutive frames by watching unlabeled videos via deep convolutional network has made significant process recently. Current state-of-the-art (SOTA) methods treat the tasks independently. One important assumption of the current depth estimation pipeline is that the scene contains no moving object, which can be complemented by the optical flow. In this paper, we propose to address the two tasks as a whole, i.e. to jointly understand per-pixel 3D geometry and motion. This also eliminates the need of static scene assumption and enforces the inherent geometrical consistency during the learning process, yielding significantly improved results for both tasks. We call our method as "Every Pixel Counts++" or "EPC++". Specifically, during training, given two consecutive frames from a video, we adopt three parallel networks to predict the camera motion (MotionNet), dense depth map (DepthNet), and per-pixel optical flow between two frames (FlowNet) respectively. Comprehensive experiments were conducted on the KITTI 2012 and KITTI 2015 datasets. Performance on the five tasks of depth estimation, optical flow estimation, odometry, moving object segmentation and scene flow estimation shows that our approach outperforms other SOTA methods, demonstrating the effectiveness of each module of our proposed method.},
	urldate = {2019-06-15},
	journal = {arXiv:1810.06125 [cs]},
	author = {Luo, Chenxu and Yang, Zhenheng and Wang, Peng and Wang, Yang and Xu, Wei and Nevatia, Ram and Yuille, Alan},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.06125},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\yida\\Zotero\\storage\\4XE36L6P\\1810.html:text/html;Luo et al_2018_Every Pixel Counts ++.pdf:C\:\\Users\\yida\\Zotero\\storage\\RK59RM7Z\\Luo et al_2018_Every Pixel Counts ++.pdf:application/pdf}
}

@article{vijayanarasimhan_sfm-net:_2017,
	title = {{SfM}-{Net}: {Learning} of {Structure} and {Motion} from {Video}},
	shorttitle = {{SfM}-{Net}},
	url = {http://arxiv.org/abs/1704.07804},
	abstract = {We propose SfM-Net, a geometry-aware neural network for motion estimation in videos that decomposes frame-to-frame pixel motion in terms of scene and object depth, camera motion and 3D object rotations and translations. Given a sequence of frames, SfM-Net predicts depth, segmentation, camera and rigid object motions, converts those into a dense frame-to-frame motion field (optical flow), differentiably warps frames in time to match pixels and back-propagates. The model can be trained with various degrees of supervision: 1) self-supervised by the re-projection photometric error (completely unsupervised), 2) supervised by ego-motion (camera motion), or 3) supervised by depth (e.g., as provided by RGBD sensors). SfM-Net extracts meaningful depth estimates and successfully estimates frame-to-frame camera rotations and translations. It often successfully segments the moving objects in the scene, even though such supervision is never provided.},
	urldate = {2019-06-18},
	journal = {arXiv:1704.07804 [cs]},
	author = {Vijayanarasimhan, Sudheendra and Ricco, Susanna and Schmid, Cordelia and Sukthankar, Rahul and Fragkiadaki, Katerina},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.07804},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1704.07804 PDF:C\:\\Users\\yida\\Zotero\\storage\\2R236WDA\\Vijayanarasimhan 等。 - 2017 - SfM-Net Learning of Structure and Motion from Vid.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\yida\\Zotero\\storage\\JXSCXNED\\1704.html:text/html;Vijayanarasimhan 等。 - 2017 - SfM-Net Learning of Structure and Motion from Vid.pdf:C\:\\Users\\yida\\Zotero\\storage\\SY5LZU7S\\Vijayanarasimhan 等。 - 2017 - SfM-Net Learning of Structure and Motion from Vid.pdf:application/pdf}
}

@article{riegler_connecting_nodate,
	title = {Connecting the {Dots}: {Learning} {Representations} for {Active} {Monocular} {Depth} {Estimation}},
	abstract = {We propose a technique for depth estimation with a monocular structured-light camera, i.e., a calibrated stereo set-up with one camera and one laser projector. Instead of formulating the depth estimation via a correspondence search problem, we show that a simple convolutional architecture is sufﬁcient for high-quality disparity estimates in this setting. As accurate ground-truth is hard to obtain, we train our model in a self-supervised fashion with a combination of photometric and geometric losses. Further, we demonstrate that the projected pattern of the structured light sensor can be reliably separated from the ambient information. This can then be used to improve depth boundaries in a weakly supervised fashion by modeling the joint statistics of image and depth edges. The model trained in this fashion compares favorably to the state-of-the-art on challenging synthetic and real-world datasets. In addition, we contribute a novel simulator, which allows to benchmark active depth prediction algorithms in controlled conditions.},
	language = {en},
	author = {Riegler, Gernot and Liao, Yiyi and Donne, Simon and Koltun, Vladlen and Geiger, Andreas},
	pages = {10},
	file = {Riegler 等。 - Connecting the Dots Learning Representations for .pdf:C\:\\Users\\yida\\Zotero\\storage\\J89CSHK6\\Riegler 等。 - Connecting the Dots Learning Representations for .pdf:application/pdf}
}

@incollection{ferrari_look_2018,
	address = {Cham},
	title = {Look {Deeper} into {Depth}: {Monocular} {Depth} {Estimation} with {Semantic} {Booster} and {Attention}-{Driven} {Loss}},
	volume = {11219},
	isbn = {978-3-030-01266-3 978-3-030-01267-0},
	shorttitle = {Look {Deeper} into {Depth}},
	url = {http://link.springer.com/10.1007/978-3-030-01267-0_4},
	abstract = {Monocular depth estimation beneﬁts greatly from learning based techniques. By studying the training data, we observe that the per-pixel depth values in existing datasets typically exhibit a long-tailed distribution. However, most previous approaches treat all the regions in the training data equally regardless of the imbalanced depth distribution, which restricts the model performance particularly on distant depth regions. In this paper, we investigate the long tail property and delve deeper into the distant depth regions (i.e. the tail part) to propose an attentiondriven loss for the network supervision. In addition, to better leverage the semantic information for monocular depth estimation, we propose a synergy network to automatically learn the information sharing strategies between the two tasks. With the proposed attention-driven loss and synergy network, the depth estimation and semantic labeling tasks can be mutually improved. Experiments on the challenging indoor dataset show that the proposed approach achieves state-of-the-art performance on both monocular depth estimation and semantic labeling tasks.},
	language = {en},
	urldate = {2019-07-02},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Jiao, Jianbo and Cao, Ying and Song, Yibing and Lau, Rynson},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01267-0_4},
	keywords = {深度估计, 语义分割},
	pages = {55--71},
	file = {Jiao 等。 - 2018 - Look Deeper into Depth Monocular Depth Estimation.pdf:C\:\\Users\\yida\\Zotero\\storage\\M8RVUQUZ\\Jiao 等。 - 2018 - Look Deeper into Depth Monocular Depth Estimation.pdf:application/pdf}
}

@inproceedings{xu_structured_2018,
	address = {Salt Lake City, UT},
	title = {Structured {Attention} {Guided} {Convolutional} {Neural} {Fields} for {Monocular} {Depth} {Estimation}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578510/},
	doi = {10.1109/CVPR.2018.00412},
	abstract = {Recent works have shown the beneﬁt of integrating Conditional Random Fields (CRFs) models into deep architectures for improving pixel-level prediction tasks. Following this line of research, in this paper we introduce a novel approach for monocular depth estimation. Similarly to previous works, our method employs a continuous CRF to fuse multi-scale information derived from different layers of a front-end Convolutional Neural Network (CNN). Differently from past works, our approach beneﬁts from a structured attention model which automatically regulates the amount of information transferred between corresponding features at different scales. Importantly, the proposed attention model is seamlessly integrated into the CRF, allowing end-to-end training of the entire architecture. Our extensive experimental evaluation demonstrates the effectiveness of the proposed method which is competitive with previous methods on the KITTI benchmark and outperforms the state of the art on the NYU Depth V2 dataset.},
	language = {en},
	urldate = {2019-08-30},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Xu, Dan and Wang, Wei and Tang, Hao and Liu, Hong and Sebe, Nicu and Ricci, Elisa},
	month = jun,
	year = {2018},
	pages = {3917--3925},
	file = {Xu 等。 - 2018 - Structured Attention Guided Convolutional Neural F.pdf:C\:\\Users\\yida\\Zotero\\storage\\U626EVG9\\Xu 等。 - 2018 - Structured Attention Guided Convolutional Neural F.pdf:application/pdf}
}

@article{casser_unsupervised_2019,
	title = {Unsupervised {Monocular} {Depth} and {Ego}-motion {Learning} with {Structure} and {Semantics}},
	url = {http://arxiv.org/abs/1906.05717},
	abstract = {We present an approach which takes advantage of both structure and semantics for unsupervised monocular learning of depth and ego-motion. More specifically, we model the motion of individual objects and learn their 3D motion vector jointly with depth and ego-motion. We obtain more accurate results, especially for challenging dynamic scenes not addressed by previous approaches. This is an extended version of Casser et al. [AAAI'19]. Code and models have been open sourced at https://sites.google.com/corp/view/struct2depth.},
	urldate = {2019-09-02},
	journal = {arXiv:1906.05717 [cs]},
	author = {Casser, Vincent and Pirk, Soeren and Mahjourian, Reza and Angelova, Anelia},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.05717},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {arXiv\:1906.05717 PDF:C\:\\Users\\yida\\Zotero\\storage\\VD9CVPYZ\\Casser 等。 - 2019 - Unsupervised Monocular Depth and Ego-motion Learni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\yida\\Zotero\\storage\\UK6UYQU2\\1906.html:text/html}
}

@article{bian_unsupervised_2019,
	title = {Unsupervised {Scale}-consistent {Depth} and {Ego}-motion {Learning} from {Monocular} {Video}},
	url = {http://arxiv.org/abs/1908.10553},
	abstract = {Recent work has shown that CNN-based depth and ego-motion estimators can be learned using unlabelled monocular videos. However, the performance is limited by unidentified moving objects that violate the underlying static scene assumption in geometric image reconstruction. More significantly, due to lack of proper constraints, networks output scale-inconsistent results over different samples, i.e., the ego-motion network cannot provide full camera trajectories over a long video sequence because of the per-frame scale ambiguity. This paper tackles these challenges by proposing a geometry consistency loss for scale-consistent predictions, and an induced self-discovered mask for handling moving objects and occlusions. Since we do not leverage multi-task learning like recent works, our framework is much simpler and more efficient. Extensive evaluation results demonstrate that our depth estimator achieves the state-of-the-art performance on the standard KITTI and Make3D datasets. Moreover, we show that our ego-motion network is able to predict a globally scale-consistent camera trajectory for long video sequences, and the resulting visual odometry accuracy is competitive with the state-of-the-art model that is trained using stereo videos. To the best of our knowledge, this is the first work to show that deep networks trained using monocular video snippets can predict globally scale-consistent camera trajectories over a long video sequence.},
	urldate = {2019-09-09},
	journal = {arXiv:1908.10553 [cs]},
	author = {Bian, Jia-Wang and Li, Zhichao and Wang, Naiyan and Zhan, Huangying and Shen, Chunhua and Cheng, Ming-Ming and Reid, Ian},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.10553},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1908.10553 PDF:C\:\\Users\\yida\\Zotero\\storage\\VJADIW52\\Bian 等。 - 2019 - Unsupervised Scale-consistent Depth and Ego-motion.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\yida\\Zotero\\storage\\5TAZZM86\\1908.html:text/html}
}

@article{xie_deep3d:_2016,
	title = {Deep3D: {Fully} {Automatic} 2D-to-3D {Video} {Conversion} with {Deep} {Convolutional} {Neural} {Networks}},
	shorttitle = {Deep3D},
	url = {http://arxiv.org/abs/1604.03650},
	abstract = {As 3D movie viewing becomes mainstream and Virtual Reality (VR) market emerges, the demand for 3D contents is growing rapidly. Producing 3D videos, however, remains challenging. In this paper we propose to use deep neural networks for automatically converting 2D videos and images to stereoscopic 3D format. In contrast to previous automatic 2D-to-3D conversion algorithms, which have separate stages and need ground truth depth map as supervision, our approach is trained end-toend directly on stereo pairs extracted from 3D movies. This novel training scheme makes it possible to exploit orders of magnitude more data and signiﬁcantly increases performance. Indeed, Deep3D outperforms baselines in both quantitative and human subject evaluations.},
	language = {en},
	urldate = {2019-09-11},
	journal = {arXiv:1604.03650 [cs]},
	author = {Xie, Junyuan and Girshick, Ross and Farhadi, Ali},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.03650},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Xie 等。 - 2016 - Deep3D Fully Automatic 2D-to-3D Video Conversion .pdf:C\:\\Users\\yida\\Zotero\\storage\\VFHGFPA4\\Xie 等。 - 2016 - Deep3D Fully Automatic 2D-to-3D Video Conversion .pdf:application/pdf}
}

@article{liu_learning_2016,
	title = {Learning {Depth} from {Single} {Monocular} {Images} {Using} {Deep} {Convolutional} {Neural} {Fields}},
	volume = {38},
	issn = {0162-8828, 2160-9292},
	url = {http://arxiv.org/abs/1502.07411},
	doi = {10.1109/TPAMI.2015.2505283},
	abstract = {In this article, we tackle the problem of depth estimation from single monocular images. Compared with depth estimation using multiple images such as stereo depth perception, depth from monocular images is much more challenging. Prior work typically focuses on exploiting geometric priors or additional sources of information, most using hand-crafted features. Recently, there is mounting evidence that features from deep convolutional neural networks (CNN) set new records for various vision applications. On the other hand, considering the continuous characteristic of the depth values, depth estimation can be naturally formulated as a continuous conditional random ﬁeld (CRF) learning problem. Therefore, here we present a deep convolutional neural ﬁeld model for estimating depths from single monocular images, aiming to jointly explore the capacity of deep CNN and continuous CRF. In particular, we propose a deep structured learning scheme which learns the unary and pairwise potentials of continuous CRF in a uniﬁed deep CNN framework. We then further propose an equally effective model based on fully convolutional networks and a novel superpixel pooling method, which is about 10 times faster, to speedup the patch-wise convolutions in the deep model. With this more efﬁcient model, we are able to design deeper networks to pursue better performance.},
	language = {en},
	number = {10},
	urldate = {2019-09-11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Liu, Fayao and Shen, Chunhua and Lin, Guosheng and Reid, Ian},
	month = oct,
	year = {2016},
	note = {arXiv: 1502.07411},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {2024--2039},
	file = {Liu 等。 - 2016 - Learning Depth from Single Monocular Images Using .pdf:C\:\\Users\\yida\\Zotero\\storage\\5BK49G8H\\Liu 等。 - 2016 - Learning Depth from Single Monocular Images Using .pdf:application/pdf}
}

@inproceedings{ranftl_dense_2016,
	title = {Dense {Monocular} {Depth} {Estimation} in {Complex} {Dynamic} {Scenes}},
	doi = {10.1109/CVPR.2016.440},
	abstract = {We present an approach to dense depth estimation from a single monocular camera that is moving through a dynamic scene. The approach produces a dense depth map from two consecutive frames. Moving objects are reconstructed along with the surrounding environment. We provide a novel motion segmentation algorithm that segments the optical flow field into a set of motion models, each with its own epipolar geometry. We then show that the scene can be reconstructed based on these motion models by optimizing a convex program. The optimization jointly reasons about the scales of different objects and assembles the scene in a common coordinate frame, determined up to a global scale. Experimental results demonstrate that the presented approach outperforms prior methods for monocular depth estimation in dynamic scenes.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Ranftl, R. and Vineet, V. and Chen, Q. and Koltun, V.},
	month = jun,
	year = {2016},
	keywords = {Estimation, image segmentation, Motion segmentation, Cameras, common coordinate frame, complex dynamic scenes, Computer vision, consecutive frames, convex program, dense depth map, dense monocular depth estimation, epipolar geometry, geometry, Image reconstruction, motion models, moving objects, optical flow field, Optical imaging, segmentation algorithm, single monocular camera, Vehicle dynamics},
	pages = {4058--4066},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\yida\\Zotero\\storage\\W2QLZA4R\\7780809.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\yida\\Zotero\\storage\\EA94KEJJ\\Ranftl 等。 - 2016 - Dense Monocular Depth Estimation in Complex Dynami.pdf:application/pdf}
}

@article{pillai_superdepth:_2018,
	title = {{SuperDepth}: {Self}-{Supervised}, {Super}-{Resolved} {Monocular} {Depth} {Estimation}},
	shorttitle = {{SuperDepth}},
	url = {http://xxx.itp.ac.cn/abs/1810.01849},
	language = {en},
	urldate = {2019-09-14},
	author = {Pillai, Sudeep and Ambrus, Rares and Gaidon, Adrien},
	month = oct,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\yida\\Zotero\\storage\\EV9ISXL2\\Pillai 等。 - 2018 - SuperDepth Self-Supervised, Super-Resolved Monocu.pdf:application/pdf;Snapshot:C\:\\Users\\yida\\Zotero\\storage\\E5YSXGX9\\1810.html:text/html}
}

@inproceedings{goroshin_unsupervised_2015,
	title = {Unsupervised {Learning} of {Spatiotemporally} {Coherent} {Metrics}},
	doi = {10.1109/ICCV.2015.465},
	abstract = {Current state-of-the-art classification and detection algorithms train deep convolutional networks using labeled data. In this work we study unsupervised feature learning with convolutional networks in the context of temporally coherent unlabeled data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity priors. We establish a connection between slow feature learning and metric learning. Using this connection we define "temporal coherence" – a criterion which can be used to set hyper-parameters in a principled and automated manner. In a transfer learning experiment, we show that the resulting encoder can be used to define a more semantically coherent metric without the use of labels.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Goroshin, R. and Bruna, J. and Tompson, J. and Eigen, D. and LeCun, Y.},
	month = dec,
	year = {2015},
	keywords = {classification algorithms, convolution, Convolution, convolutional pooling auto-encoder, deep convolutional networks, detection algorithms, Dictionaries, Feature extraction, hyperparameters, labeled data, Measurement, metric learning, spatiotemporally coherent metrics, Training, transfer learning, unsupervised feature learning, unsupervised learning, Unsupervised learning, video data, video frames, Video sequences, video signal processing},
	pages = {4086--4093},
	file = {Goroshin et al_2015_Unsupervised Learning of Spatiotemporally Coherent Metrics.pdf:C\:\\Users\\yida\\Zotero\\storage\\BEEDND8T\\Goroshin et al_2015_Unsupervised Learning of Spatiotemporally Coherent Metrics.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\yida\\Zotero\\storage\\BI6MNDUY\\7410822.html:text/html}
}

@article{zhou_unsupervised_2019,
	title = {Unsupervised {High}-{Resolution} {Depth} {Learning} {From} {Videos} {With} {Dual} {Networks}},
	url = {http://arxiv.org/abs/1910.08897},
	abstract = {Unsupervised depth learning takes the appearance difference between a target view and a view synthesized from its adjacent frame as supervisory signal. Since the supervisory signal only comes from images themselves, the resolution of training data signiﬁcantly impacts the performance. High-resolution images contain more ﬁne-grained details and provide more accurate supervisory signal. However, due to the limitation of memory and computation power, the original images are typically down-sampled during training, which suffers heavy loss of details and disparity accuracy. In order to fully explore the information contained in high-resolution data, we propose a simple yet effective dual networks architecture, which can directly take highresolution images as input and generate high-resolution and high-accuracy depth map efﬁciently. We also propose a Self-assembled Attention (SA-Attention) module to handle low-texture region. The evaluation on the benchmark KITTI and Make3D datasets demonstrates that our method achieves state-of-the-art results in the monocular depth estimation task.},
	language = {en},
	urldate = {2019-10-29},
	journal = {arXiv:1910.08897 [cs]},
	author = {Zhou, Junsheng and Wang, Yuwang and Qin, Kaihuai and Zeng, Wenjun},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.08897},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Zhou 等。 - 2019 - Unsupervised High-Resolution Depth Learning From V.pdf:C\:\\Users\\yida\\Zotero\\storage\\9ENGQVJJ\\Zhou 等。 - 2019 - Unsupervised High-Resolution Depth Learning From V.pdf:application/pdf}
}

@article{zhou_moving_2019,
	title = {Moving {Indoor}: {Unsupervised} {Video} {Depth} {Learning} in {Challenging} {Environments}},
	shorttitle = {Moving {Indoor}},
	url = {http://arxiv.org/abs/1910.08898},
	abstract = {Recently unsupervised learning of depth from videos has made remarkable progress and the results are comparable to fully supervised methods in outdoor scenes like KITTI. However, there still exist great challenges when directly applying this technology in indoor environments, e.g., large areas of non-texture regions like white wall, more complex ego-motion of handheld camera, transparent glasses and shiny objects. To overcome these problems, we propose a new optical-ﬂow based training paradigm which reduces the difﬁculty of unsupervised learning by providing a clearer training target and handles the non-texture regions. Our experimental evaluation demonstrates that the result of our method is comparable to fully supervised methods on the NYU Depth V2 benchmark. To the best of our knowledge, this is the ﬁrst quantitative result of purely unsupervised learning method reported on indoor datasets.},
	language = {en},
	urldate = {2019-10-29},
	journal = {arXiv:1910.08898 [cs]},
	author = {Zhou, Junsheng and Wang, Yuwang and Qin, Kaihuai and Zeng, Wenjun},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.08898},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {1910.pdf:C\:\\Users\\yida\\Zotero\\storage\\8MJHD4HQ\\1910.pdf:application/pdf}
}

@article{li_pose_2019,
	title = {Pose {Graph} {Optimization} for {Unsupervised} {Monocular} {Visual} {Odometry}},
	url = {http://arxiv.org/abs/1903.06315},
	abstract = {Unsupervised Learning based monocular visual odometry (VO) has lately drawn signiﬁcant attention for its potential in label-free leaning ability and robustness to camera parameters and environmental variations. However, partially due to the lack of drift correction technique, these methods are still by far less accurate than geometric approaches for largescale odometry estimation. In this paper, we propose to leverage graph optimization and loop closure detection to overcome limitations of unsupervised learning based monocular visual odometry. To this end, we propose a hybrid VO system which combines an unsupervised monocular VO called NeuralBundler with a pose graph optimization back-end. NeuralBundler is a neural network architecture that uses temporal and spatial photometric loss as main supervision and generates a windowed pose graph consists of multi-view 6DoF constraints. We propose a novel pose cycle consistency loss to relieve the tensions in the windowed pose graph, leading to improved performance and robustness. In the back-end, a global pose graph is built from local and loop 6DoF constraints estimated by NeuralBundler, and is optimized over SE(3). Empirical evaluation on the KITTI odometry dataset demonstrates that 1) NeuralBundler achieves state-of-the-art performance on unsupervised monocular VO estimation, and 2) our whole approach can achieve efﬁcient loop closing and show favorable overall translational accuracy compared to established monocular SLAM systems.},
	language = {en},
	urldate = {2019-11-01},
	journal = {arXiv:1903.06315 [cs]},
	author = {Li, Yang and Ushiku, Yoshitaka and Harada, Tatsuya},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.06315},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Li 等。 - 2019 - Pose Graph Optimization for Unsupervised Monocular.pdf:C\:\\Users\\yida\\Zotero\\storage\\KC8KW5K6\\Li 等。 - 2019 - Pose Graph Optimization for Unsupervised Monocular.pdf:application/pdf}
}

@inproceedings{kendall_posenet:_2015,
	address = {Santiago, Chile},
	title = {{PoseNet}: {A} {Convolutional} {Network} for {Real}-{Time} 6-{DOF} {Camera} {Relocalization}},
	isbn = {978-1-4673-8391-2},
	shorttitle = {{PoseNet}},
	url = {http://ieeexplore.ieee.org/document/7410693/},
	doi = {10.1109/ICCV.2015.336},
	language = {en},
	urldate = {2019-11-04},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Kendall, Alex and Grimes, Matthew and Cipolla, Roberto},
	month = dec,
	year = {2015},
	pages = {2938--2946},
	file = {Kendall 等。 - 2015 - PoseNet A Convolutional Network for Real-Time 6-D.pdf:C\:\\Users\\yida\\Zotero\\storage\\4CYLVU4P\\Kendall 等。 - 2015 - PoseNet A Convolutional Network for Real-Time 6-D.pdf:application/pdf}
}


@incollection{jaderberg_spatial_2015,
	title = {Spatial {Transformer} {Networks}},
	url = {http://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf},
	urldate = {2019-06-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and kavukcuoglu, koray},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {2017--2025},
	file = {NIPS Full Text PDF:C\:\\Users\\yida\\Zotero\\storage\\VYWEV8LT\\Jaderberg 等。 - 2015 - Spatial Transformer Networks.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\yida\\Zotero\\storage\\IT56AYNE\\5854-spatial-transformer-networks.html:text/html}
}
